
NOTE: 

My branch predictor uses a local one-step history predictor, ie. if that branch is taken, it most likely will be taken again. It falls back on a static not-taken predictor when there is no history to rely on. 

I believe that this type of predictor is most efficient, because of the shortness of the tests coupled with the nature of the jeq instruction. 

Since many of the tests are so short, using very few unique 'jeq' instructions, implementing a two-step history predictor will actually increase CPI due to the warm up time. Having a global history predictor would only obfuscate the predictions further. However, if more of the tests were lengthier and had more branches, it would be beneficial to have both a local and global history predictor that took more than one step to establish a pattern.

Because 'jeq' only can jump forward, the static predictor of 'always not taken' is almost always the optimal prediction, except in a few cases (which the history predictor fixes). More often than not, 'jeq' is used to exit a loop or exit the program, and thus is not used often enough to neccesitate dynamic branch prediction. In the few cases that the dynamic branch prediction is activated, the impact is quite low (only a few cycles are saved). 

Lastly, the impact of branch prediction is very low for p7, because I had already implemented an early branch execution for p5. This implementation would check early before the writeback stage, and take the branch once it was 100% verified to be taken. Here is a diagram for clarity:


	Prediction
		|
		v
	F 	D 	R 	A 	M 	W
				^
				|
			Early execution
			
Thus, not-taken is almost always the optimal branch prediction. Assuming in the tests, there is a 50% of taken and a 50% chance of not taken (which is untrue -- not taken is much more common), it is inefficient to predict true unless the prediction can be made with more than ~73.7% accuracy. 	
Essentially: branch prediction is efficient where applicable, but if not, early execution and static prediction ensures that the penalty taken without is not too large.


-------------------------------------------------------------------------------------

           P6 CPI       p7 CPI       Impact of branch predictor

test1 ... 1.438356		1.383562	 Branch fail prediction allowed for earlier jump

test2 ... 1.382353		1.382353

test3 ... 1.151515		1.151515

test4 ... 1.217391		1.217391
					
test5 ... 1.431373		1.392157	 Branch fail prediction allowed for earlier jump	  

test6 ... 1.636364		1.636364

test7 ... 1.666667		1.666667

test8 ... 2.125000		2.125000

test9 ... 2.069277		1.942943	 Branch fail prediction allowed for earlier jump

testa ... 1.166667		1.166667

testb ... 2.514286		1.611111	 True prediction greatly decreased cycle count

testc ... 1.517241		1.448276	 Branch fail prediction allowed for earlier jump

testd ... 1.294118		1.294118

teste ... 1.277778		1.277778

testf ... 1.277778		1.277778

testg ... 2.250000		2.250000

testh ... 3.500000		3.500000

testi ... 1.200000		1.200000

testj ... 2.800000		2.800000

testk ... 2.000000		2.000000

testl ... 1.296296		1.296296

testm ... 2.375000		1.875000	 Changed to allow early jump at decode stage

testn ... 1.666667		2.333333	 Cycle counter revised

testo ... 1.566667		1.566667

testv ... 1.451613		1.451613

testw ... 1.998691		1.998691

testz ... 1.889371		1.889371

testj-spam 1.534884		2.953125	Note that this is my own test devised to test branch 											efficiency. It is similar to testb in that there is a jeq 										that will always jump. CPI was decreased in half -- this is 									the most optimal scenario for branch prediction.
